{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T04:37:55.853343Z",
     "iopub.status.busy": "2025-06-28T04:37:55.853109Z",
     "iopub.status.idle": "2025-06-28T04:37:55.859339Z",
     "shell.execute_reply": "2025-06-28T04:37:55.858747Z",
     "shell.execute_reply.started": "2025-06-28T04:37:55.853317Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/kaggle/input/fu-net/FuNET-C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T04:37:59.000262Z",
     "iopub.status.busy": "2025-06-28T04:37:58.999986Z",
     "iopub.status.idle": "2025-06-28T04:40:34.614152Z",
     "shell.execute_reply": "2025-06-28T04:40:34.612694Z",
     "shell.execute_reply.started": "2025-06-28T04:37:59.000241Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-28 04:38:00.709186: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751085480.906918      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751085480.958752      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "I0000 00:00:1751085494.681357      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1751085494.706445      35 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n",
      "I0000 00:00:1751085505.831693      94 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sup] 50: Train=0.9007, Test=0.7070\n",
      "[Sup] 100: Train=0.9871, Test=0.7959\n",
      "[Sup] 150: Train=1.0000, Test=0.8246\n",
      "[Sup] 200: Train=1.0000, Test=0.8197\n",
      "[SimCLR] Epoch 50: Probe Train Acc = 0.5482, Probe Test Acc = 0.4976\n",
      "[SimCLR] Epoch 100: Probe Train Acc = 0.5799, Probe Test Acc = 0.5069\n",
      "[SimCLR] Epoch 150: Probe Train Acc = 0.6072, Probe Test Acc = 0.5184\n",
      "[SimCLR] Epoch 200: Probe Train Acc = 0.6129, Probe Test Acc = 0.5250\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "import tensorflow as tf\n",
    "from tf_utils import random_mini_batches_GCN1, convert_to_one_hot\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create logisticâ€regression probe once\n",
    "probe_clf = LogisticRegression(max_iter=2000)\n",
    "\n",
    "# TF1.x setup\n",
    "tf.compat.v1.reset_default_graph()\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# --- Placeholders ---\n",
    "spec_ph   = tf.compat.v1.placeholder(tf.float32, [None, 200],    name=\"spec\")\n",
    "patch_ph  = tf.compat.v1.placeholder(tf.float32, [None, 7*7*200], name=\"patch\")\n",
    "label_ph  = tf.compat.v1.placeholder(tf.float32, [None, 16],     name=\"label\")\n",
    "lap_ph    = tf.compat.v1.placeholder(tf.float32, [None, None],   name=\"lap\")\n",
    "\n",
    "# For SimCLR\n",
    "spec1_ph, spec2_ph = [\n",
    "    tf.compat.v1.placeholder(tf.float32, [None, 200], name=n)\n",
    "    for n in (\"spec1\",\"spec2\")\n",
    "]\n",
    "patch1_ph, patch2_ph = [\n",
    "    tf.compat.v1.placeholder(tf.float32, [None, 7*7*200], name=n)\n",
    "    for n in (\"patch1\",\"patch2\")\n",
    "]\n",
    "\n",
    "# --- FuNet-C variables ---\n",
    "with tf.compat.v1.variable_scope(\"FUNET\"):\n",
    "    Wg = tf.compat.v1.get_variable(\"x_w1\",    [200,128], initializer=tf.keras.initializers.GlorotUniform())\n",
    "    bg = tf.compat.v1.get_variable(\"x_b1\",    [128],    initializer=tf.zeros_initializer())\n",
    "    Wc1 = tf.compat.v1.get_variable(\"x_conv_w1\",[3,3,200,32], initializer=tf.keras.initializers.GlorotUniform())\n",
    "    bc1 = tf.compat.v1.get_variable(\"x_conv_b1\",[32],       initializer=tf.zeros_initializer())\n",
    "    Wc2 = tf.compat.v1.get_variable(\"x_conv_w2\",[3,3,32,64],  initializer=tf.keras.initializers.GlorotUniform())\n",
    "    bc2 = tf.compat.v1.get_variable(\"x_conv_b2\",[64],       initializer=tf.zeros_initializer())\n",
    "    Wc3 = tf.compat.v1.get_variable(\"x_conv_w3\",[1,1,64,128],initializer=tf.keras.initializers.GlorotUniform())\n",
    "    bc3 = tf.compat.v1.get_variable(\"x_conv_b3\",[128],      initializer=tf.zeros_initializer())\n",
    "    Wf1 = tf.compat.v1.get_variable(\"x_jw1\",[256,128],       initializer=tf.keras.initializers.GlorotUniform())\n",
    "    bf1 = tf.compat.v1.get_variable(\"x_jb1\",[128],           initializer=tf.zeros_initializer())\n",
    "    Wf2 = tf.compat.v1.get_variable(\"x_jw2\",[128,16],        initializer=tf.keras.initializers.GlorotUniform())\n",
    "    bf2 = tf.compat.v1.get_variable(\"x_jb2\",[16],            initializer=tf.zeros_initializer())\n",
    "\n",
    "# --- Encoder ---\n",
    "def funet_encode(spec, patch, lap):\n",
    "    h = tf.matmul(spec, Wg) + bg\n",
    "    g = tf.matmul(lap, h)\n",
    "    r  = tf.reshape(patch, [-1,7,7,200])\n",
    "    c1 = tf.nn.relu(tf.nn.max_pool2d(tf.nn.conv2d(r,Wc1,[1,1,1,1],\"SAME\")+bc1,2,2,\"SAME\"))\n",
    "    c2 = tf.nn.relu(tf.nn.max_pool2d(tf.nn.conv2d(c1,Wc2,[1,1,1,1],\"SAME\")+bc2,2,2,\"SAME\"))\n",
    "    c3 = tf.nn.relu(tf.nn.max_pool2d(tf.nn.conv2d(c2,Wc3,[1,1,1,1],\"SAME\")+bc3,2,2,\"SAME\"))\n",
    "    c  = tf.reshape(c3,[-1,128])\n",
    "    j  = tf.concat([g,c],1)\n",
    "    f1 = tf.nn.relu(tf.matmul(j,Wf1)+bf1)\n",
    "    logits = tf.matmul(f1,Wf2)+bf2\n",
    "    return logits, j\n",
    "\n",
    "# --- Supervised head ---\n",
    "logits, joint_feats = funet_encode(spec_ph, patch_ph, lap_ph)\n",
    "sup_loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=label_ph)\n",
    ")\n",
    "sup_opt = tf.compat.v1.train.AdamOptimizer(1e-3).minimize(sup_loss)\n",
    "sup_acc = tf.reduce_mean(tf.cast(\n",
    "    tf.equal(tf.argmax(logits,1), tf.argmax(label_ph,1)), tf.float32\n",
    "))\n",
    "\n",
    "# --- SimCLR augmentations ---\n",
    "a1 = spec1_ph + tf.random.normal(tf.shape(spec1_ph), stddev=0.01)\n",
    "a2 = spec2_ph + tf.random.normal(tf.shape(spec2_ph), stddev=0.01)\n",
    "def aug_patch(ph):\n",
    "    b = tf.shape(ph)[0]\n",
    "    r = tf.reshape(ph, [b,7,7,200])\n",
    "    cr= tf.image.random_crop(r,[b,5,5,200])\n",
    "    cr= tf.image.resize(cr,[7,7])+tf.random.normal(tf.shape(r),stddev=0.01)\n",
    "    return tf.reshape(cr,[b,-1])\n",
    "ap1 = aug_patch(patch1_ph)\n",
    "ap2 = aug_patch(patch2_ph)\n",
    "\n",
    "# Reuse FUNET\n",
    "with tf.compat.v1.variable_scope(\"FUNET\", reuse=True):\n",
    "    pass\n",
    "\n",
    "_, h1 = funet_encode(a1, ap1, lap_ph)\n",
    "_, h2 = funet_encode(a2, ap2, lap_ph)\n",
    "\n",
    "# --- Projection head variables ---\n",
    "with tf.compat.v1.variable_scope(\"SIMCLR_HEAD\"):\n",
    "    # two-layer MLP\n",
    "    Wp1 = tf.compat.v1.get_variable(\"Wp1\", [256,256], initializer=tf.keras.initializers.GlorotUniform())\n",
    "    bp1 = tf.compat.v1.get_variable(\"bp1\", [256], initializer=tf.zeros_initializer())\n",
    "    Wp2 = tf.compat.v1.get_variable(\"Wp2\", [256,128], initializer=tf.keras.initializers.GlorotUniform())\n",
    "    bp2 = tf.compat.v1.get_variable(\"bp2\", [128], initializer=tf.zeros_initializer())\n",
    "\n",
    "def project(h):\n",
    "    x = tf.nn.relu(tf.matmul(h, Wp1) + bp1)\n",
    "    return tf.matmul(x, Wp2) + bp2\n",
    "\n",
    "z1 = project(h1)\n",
    "z2 = project(h2)\n",
    "\n",
    "# --- NT-Xent loss ---\n",
    "z1n = tf.math.l2_normalize(z1,1)\n",
    "z2n = tf.math.l2_normalize(z2,1)\n",
    "sim  = tf.matmul(z1n, z2n, transpose_b=True)\n",
    "labels_con = tf.range(tf.shape(sim)[0])\n",
    "sim_loss = tf.reduce_mean(\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits(logits=sim, labels=labels_con)\n",
    ")\n",
    "\n",
    "# --- SimCLR optimizer on just projection head ---\n",
    "proj_vars = [Wp1, bp1, Wp2, bp2]\n",
    "sim_opt = tf.compat.v1.train.AdamOptimizer(1e-3).minimize(sim_loss, var_list=proj_vars)\n",
    "\n",
    "# --- Run session ---\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "    # Load data\n",
    "    X_tr = scio.loadmat('/kaggle/input/fu-net/FuNET-C/Train_X.mat')['Train_X']\n",
    "    P_tr = scio.loadmat('/kaggle/input/fu-net/FuNET-C/X_train.mat')['X_train']\n",
    "    L_tr = scio.loadmat('/kaggle/input/fu-net/FuNET-C/Train_L.mat')['Train_L']\n",
    "    Ytr  = convert_to_one_hot(scio.loadmat('/kaggle/input/fu-net/FuNET-C/TrLabel.mat')['TrLabel']-1,16).T\n",
    "\n",
    "    X_te = scio.loadmat('/kaggle/input/fu-net/FuNET-C/Test_X.mat')['Test_X']\n",
    "    P_te = scio.loadmat('/kaggle/input/fu-net/FuNET-C/X_test.mat')['X_test']\n",
    "    L_te = scio.loadmat('/kaggle/input/fu-net/FuNET-C/Test_L.mat')['Test_L']\n",
    "    Yte  = convert_to_one_hot(scio.loadmat('/kaggle/input/fu-net/FuNET-C/TeLabel.mat')['TeLabel']-1,16).T\n",
    "\n",
    "    # Supervised\n",
    "    for e in range(1,201):\n",
    "        mbs = random_mini_batches_GCN1(X_tr, P_tr, Ytr, L_tr, 32, e)\n",
    "        for bx,bp,by,bl in mbs:\n",
    "            sess.run(sup_opt, feed_dict={spec_ph:bx, patch_ph:bp, label_ph:by, lap_ph:bl})\n",
    "        if e%50==0:\n",
    "            tr_acc = sess.run(sup_acc, feed_dict={spec_ph:X_tr, patch_ph:P_tr, label_ph:Ytr, lap_ph:L_tr})\n",
    "            te_acc = sess.run(sup_acc, feed_dict={spec_ph:X_te, patch_ph:P_te, label_ph:Yte, lap_ph:L_te})\n",
    "            print(f\"[Sup] {e}: Train={tr_acc:.4f}, Test={te_acc:.4f}\")\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        # 1) SimCLR training\n",
    "        idx = np.random.permutation(X_tr.shape[0])\n",
    "        for i in range(X_tr.shape[0] // 32):\n",
    "            bi = idx[i*32:(i+1)*32]\n",
    "            sess.run(sim_opt, feed_dict={\n",
    "                spec1_ph: X_tr[bi],\n",
    "                spec2_ph: X_tr[bi],\n",
    "                patch1_ph: P_tr[bi],\n",
    "                patch2_ph: P_tr[bi],\n",
    "                lap_ph:     L_tr[np.ix_(bi, bi)]\n",
    "            })\n",
    "    \n",
    "        # Only do probe at epochs 50,100,150,200\n",
    "        if epoch % 50 == 0:\n",
    "            # 2) Extract features\n",
    "            F_tr = sess.run(z1n, feed_dict={spec1_ph: X_tr, patch1_ph: P_tr, lap_ph: L_tr})\n",
    "            F_te = sess.run(z1n, feed_dict={spec1_ph: X_te, patch1_ph: P_te, lap_ph: L_te})\n",
    "    \n",
    "            # 3) Fit & evaluate\n",
    "            y_tr = np.argmax(Ytr, axis=1)\n",
    "            y_te = np.argmax(Yte, axis=1)\n",
    "    \n",
    "            probe_clf.fit(F_tr, y_tr)\n",
    "            tr_acc = probe_clf.score(F_tr, y_tr)\n",
    "            te_acc = probe_clf.score(F_te, y_te)\n",
    "    \n",
    "            print(f\"[SimCLR] Epoch {epoch}: Probe Train Acc = {tr_acc:.4f}, Probe Test Acc = {te_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T04:40:38.295325Z",
     "iopub.status.busy": "2025-06-28T04:40:38.295031Z",
     "iopub.status.idle": "2025-06-28T04:42:08.166065Z",
     "shell.execute_reply": "2025-06-28T04:42:08.165323Z",
     "shell.execute_reply.started": "2025-06-28T04:40:38.295304Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1751085646.063625      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SimCLR] Epoch 50, Loss=2.5703\n",
      "[SimCLR] Epoch 100, Loss=2.6167\n",
      "[SimCLR] Epoch 150, Loss=2.5745\n",
      "[SimCLR] Epoch 200, Loss=2.5675\n",
      "[Fine-Tune] Epoch 50: Train Acc=0.9194, Test Acc=0.7329\n",
      "[Fine-Tune] Epoch 100: Train Acc=0.9986, Test Acc=0.8042\n",
      "[Fine-Tune] Epoch 150: Train Acc=1.0000, Test Acc=0.8175\n",
      "[Fine-Tune] Epoch 200: Train Acc=1.0000, Test Acc=0.8106\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "import tensorflow as tf\n",
    "from tf_utils import random_mini_batches_GCN1, convert_to_one_hot\n",
    "\n",
    "# TF1.x setup\n",
    "tf.compat.v1.reset_default_graph()\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "spec_ph   = tf.compat.v1.placeholder(tf.float32, [None, 200],    name=\"spec\")\n",
    "patch_ph  = tf.compat.v1.placeholder(tf.float32, [None, 7*7*200], name=\"patch\")\n",
    "label_ph  = tf.compat.v1.placeholder(tf.float32, [None, 16],     name=\"label\")\n",
    "lap_ph    = tf.compat.v1.placeholder(tf.float32, [None, None],   name=\"lap\")\n",
    "\n",
    "# For SimCLR augmentations\n",
    "spec1_ph, spec2_ph = [\n",
    "    tf.compat.v1.placeholder(tf.float32, [None,200], name=n)\n",
    "    for n in (\"spec1\",\"spec2\")\n",
    "]\n",
    "patch1_ph, patch2_ph = [\n",
    "    tf.compat.v1.placeholder(tf.float32, [None,7*7*200], name=n)\n",
    "    for n in (\"patch1\",\"patch2\")\n",
    "]\n",
    "\n",
    "with tf.compat.v1.variable_scope(\"FUNET\"):\n",
    "    # GCN\n",
    "    Wg  = tf.compat.v1.get_variable(\"x_w1\",    [200,128], initializer=tf.keras.initializers.GlorotUniform())\n",
    "    bg  = tf.compat.v1.get_variable(\"x_b1\",    [128],    initializer=tf.zeros_initializer())\n",
    "    # CNN\n",
    "    Wc1 = tf.compat.v1.get_variable(\"x_conv_w1\",[3,3,200,32], initializer=tf.keras.initializers.GlorotUniform())\n",
    "    bc1 = tf.compat.v1.get_variable(\"x_conv_b1\",[32],       initializer=tf.zeros_initializer())\n",
    "    Wc2 = tf.compat.v1.get_variable(\"x_conv_w2\",[3,3,32,64],  initializer=tf.keras.initializers.GlorotUniform())\n",
    "    bc2 = tf.compat.v1.get_variable(\"x_conv_b2\",[64],       initializer=tf.zeros_initializer())\n",
    "    Wc3 = tf.compat.v1.get_variable(\"x_conv_w3\",[1,1,64,128],initializer=tf.keras.initializers.GlorotUniform())\n",
    "    bc3 = tf.compat.v1.get_variable(\"x_conv_b3\",[128],      initializer=tf.zeros_initializer())\n",
    "    # Fusion MLP\n",
    "    Wf1 = tf.compat.v1.get_variable(\"x_jw1\",[256,128],       initializer=tf.keras.initializers.GlorotUniform())\n",
    "    bf1 = tf.compat.v1.get_variable(\"x_jb1\",[128],           initializer=tf.zeros_initializer())\n",
    "    Wf2 = tf.compat.v1.get_variable(\"x_jw2\",[128,16],        initializer=tf.keras.initializers.GlorotUniform())\n",
    "    bf2 = tf.compat.v1.get_variable(\"x_jb2\",[16],            initializer=tf.zeros_initializer())\n",
    "\n",
    "# --- Encoder function ---\n",
    "def funet_encode(spec, patch, lap):\n",
    "    # GCN branch\n",
    "    h = tf.matmul(spec, Wg) + bg\n",
    "    g = tf.matmul(lap, h)                # [batch,128]\n",
    "    # CNN branch\n",
    "    r  = tf.reshape(patch, [-1,7,7,200])\n",
    "    c1 = tf.nn.relu(tf.nn.max_pool2d(tf.nn.conv2d(r,Wc1,[1,1,1,1],\"SAME\")+bc1,2,2,\"SAME\"))\n",
    "    c2 = tf.nn.relu(tf.nn.max_pool2d(tf.nn.conv2d(c1,Wc2,[1,1,1,1],\"SAME\")+bc2,2,2,\"SAME\"))\n",
    "    c3 = tf.nn.relu(tf.nn.max_pool2d(tf.nn.conv2d(c2,Wc3,[1,1,1,1],\"SAME\")+bc3,2,2,\"SAME\"))\n",
    "    c  = tf.reshape(c3, [-1,128])        # [batch,128]\n",
    "    # Fusion\n",
    "    j  = tf.concat([g,c], axis=1)        # [batch,256]\n",
    "    f1 = tf.nn.relu(tf.matmul(j,Wf1) + bf1)\n",
    "    logits = tf.matmul(f1,Wf2) + bf2\n",
    "    return logits, j\n",
    "\n",
    "# Augmentations\n",
    "a1 = spec1_ph + tf.random.normal(tf.shape(spec1_ph), stddev=0.01)\n",
    "a2 = spec2_ph + tf.random.normal(tf.shape(spec2_ph), stddev=0.01)\n",
    "def aug_patch(ph):\n",
    "    b = tf.shape(ph)[0]\n",
    "    r = tf.reshape(ph,[b,7,7,200])\n",
    "    cr= tf.image.random_crop(r,[b,5,5,200])\n",
    "    cr= tf.image.resize(cr,[7,7]) + tf.random.normal(tf.shape(r),stddev=0.01)\n",
    "    return tf.reshape(cr,[b,-1])\n",
    "ap1, ap2 = aug_patch(patch1_ph), aug_patch(patch2_ph)\n",
    "\n",
    "# Reuse FUNET to encode\n",
    "with tf.compat.v1.variable_scope(\"FUNET\", reuse=True):\n",
    "    pass\n",
    "_, h1 = funet_encode(a1, ap1, lap_ph)\n",
    "_, h2 = funet_encode(a2, ap2, lap_ph)\n",
    "\n",
    "# Projection MLP under its own scope\n",
    "with tf.compat.v1.variable_scope(\"SIMCLR_HEAD\"):\n",
    "    Wp1 = tf.compat.v1.get_variable(\"Wp1\",[256,256], initializer=tf.keras.initializers.GlorotUniform())\n",
    "    bp1 = tf.compat.v1.get_variable(\"bp1\",[256], initializer=tf.zeros_initializer())\n",
    "    Wp2 = tf.compat.v1.get_variable(\"Wp2\",[256,128], initializer=tf.keras.initializers.GlorotUniform())\n",
    "    bp2 = tf.compat.v1.get_variable(\"bp2\",[128], initializer=tf.zeros_initializer())\n",
    "\n",
    "def project(h):\n",
    "    x = tf.nn.relu(tf.matmul(h,Wp1) + bp1)\n",
    "    return tf.matmul(x,Wp2) + bp2\n",
    "\n",
    "z1, z2 = project(h1), project(h2)\n",
    "\n",
    "# NT-Xent loss\n",
    "z1n = tf.math.l2_normalize(z1,1)\n",
    "z2n = tf.math.l2_normalize(z2,1)\n",
    "sim  = tf.matmul(z1n, z2n, transpose_b=True)\n",
    "labels_sim = tf.range(tf.shape(sim)[0])\n",
    "sim_loss = tf.reduce_mean(\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits(logits=sim, labels=labels_sim)\n",
    ")\n",
    "\n",
    "# SimCLR optimizer on only projection head\n",
    "proj_vars = [Wp1,bp1,Wp2,bp2]\n",
    "sim_opt = tf.compat.v1.train.AdamOptimizer(1e-3).minimize(sim_loss, var_list=proj_vars)\n",
    "\n",
    "# We'll reuse funet_encode on spec_ph/patch_ph\n",
    "logits, joint_feats = funet_encode(spec_ph, patch_ph, lap_ph)\n",
    "sup_loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=label_ph)\n",
    ")\n",
    "sup_opt = tf.compat.v1.train.AdamOptimizer(1e-3).minimize(sup_loss)\n",
    "sup_acc = tf.reduce_mean(tf.cast(\n",
    "    tf.equal(tf.argmax(logits,1), tf.argmax(label_ph,1)), tf.float32\n",
    "))\n",
    "\n",
    "\n",
    "X_tr = scio.loadmat('/kaggle/input/fu-net/FuNET-C/Train_X.mat')['Train_X']\n",
    "P_tr = scio.loadmat('/kaggle/input/fu-net/FuNET-C/X_train.mat')['X_train']\n",
    "L_tr = scio.loadmat('/kaggle/input/fu-net/FuNET-C/Train_L.mat')['Train_L']\n",
    "Ytr  = convert_to_one_hot(scio.loadmat('/kaggle/input/fu-net/FuNET-C/TrLabel.mat')['TrLabel']-1,16).T\n",
    "\n",
    "X_te = scio.loadmat('/kaggle/input/fu-net/FuNET-C/Test_X.mat')['Test_X']\n",
    "P_te = scio.loadmat('/kaggle/input/fu-net/FuNET-C/X_test.mat')['X_test']\n",
    "L_te = scio.loadmat('/kaggle/input/fu-net/FuNET-C/Test_L.mat')['Test_L']\n",
    "Yte  = convert_to_one_hot(scio.loadmat('/kaggle/input/fu-net/FuNET-C/TeLabel.mat')['TeLabel']-1,16).T\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "    # 1) SimCLR pre-training\n",
    "    for epoch in range(1, 201):\n",
    "        idx = np.random.permutation(X_tr.shape[0])\n",
    "        for i in range(X_tr.shape[0]//32):\n",
    "            bi = idx[i*32:(i+1)*32]\n",
    "            sess.run(sim_opt, feed_dict={\n",
    "                spec1_ph: X_tr[bi], spec2_ph: X_tr[bi],\n",
    "                patch1_ph:P_tr[bi], patch2_ph:P_tr[bi],\n",
    "                lap_ph:    L_tr[np.ix_(bi,bi)]\n",
    "            })\n",
    "        if epoch % 50 == 0:\n",
    "            loss_val = sess.run(sim_loss, feed_dict={\n",
    "                spec1_ph: X_tr[bi], spec2_ph: X_tr[bi],\n",
    "                patch1_ph:P_tr[bi], patch2_ph:P_tr[bi],\n",
    "                lap_ph:    L_tr[np.ix_(bi,bi)]\n",
    "            })\n",
    "            print(f\"[SimCLR] Epoch {epoch}, Loss={loss_val:.4f}\")\n",
    "\n",
    "    # 2) Fine-tune end-to-end supervised\n",
    "    for epoch in range(1, 201):\n",
    "        mbs = random_mini_batches_GCN1(X_tr, P_tr, Ytr, L_tr, 32, epoch)\n",
    "        for bx,bp,by,bl in mbs:\n",
    "            sess.run(sup_opt, feed_dict={\n",
    "                spec_ph: bx, patch_ph: bp, label_ph: by, lap_ph: bl\n",
    "            })\n",
    "        if epoch % 50 == 0:\n",
    "            tr_acc = sess.run(sup_acc, feed_dict={\n",
    "                spec_ph: X_tr, patch_ph: P_tr,\n",
    "                label_ph: Ytr,  lap_ph: L_tr\n",
    "            })\n",
    "            te_acc = sess.run(sup_acc, feed_dict={\n",
    "                spec_ph: X_te, patch_ph: P_te,\n",
    "                label_ph: Yte,  lap_ph: L_te\n",
    "            })\n",
    "            print(f\"[Fine-Tune] Epoch {epoch}: Train Acc={tr_acc:.4f}, Test Acc={te_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7606355,
     "sourceId": 12082878,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
